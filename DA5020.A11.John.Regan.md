DA5020.A11.John.Regan.Rmd
================
Regan, JD
2023-08-28

## DA5020 – Assignment 11 

This assignment provides you with an opportunity to implement the kNN
algorithm and identify suitable values of k. In this exercise you will
build a k-nearest neighbor classifier to predict the onset of diabetes
using the Pima Indians Diabetes Database. The dataset contains the
following explanatory variables: number of pregnancies, glucose, blood
pressure, skin thickness, insulin, BMI, diabetes pedigree function and
age. The response variable is Outcome, which indicates whether or not
the patient has diabetes.  

Question 1 — (5 points)  
Load the diabetes dataset “diabetes.csv”, inspect the data and gather
any relevant summary statistics.  

``` r
diab <- read_csv("diabetes.csv")
```

    ## Rows: 768 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (9): Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, D...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#checking for na values
colSums(is.na(diab))
```

    ##              Pregnancies                  Glucose            BloodPressure 
    ##                        0                        0                        0 
    ##            SkinThickness                  Insulin                      BMI 
    ##                        0                        0                        0 
    ## DiabetesPedigreeFunction                      Age                  Outcome 
    ##                        0                        0                        0

``` r
summary(diab)
```

    ##   Pregnancies        Glucose      BloodPressure    SkinThickness  
    ##  Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  
    ##  1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  
    ##  Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  
    ##  Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  
    ##  3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  
    ##  Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  
    ##     Insulin           BMI        DiabetesPedigreeFunction      Age       
    ##  Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  
    ##  1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  
    ##  Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  
    ##  Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  
    ##  3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  
    ##  Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  
    ##     Outcome     
    ##  Min.   :0.000  
    ##  1st Qu.:0.000  
    ##  Median :0.000  
    ##  Mean   :0.349  
    ##  3rd Qu.:1.000  
    ##  Max.   :1.000

``` r
str(diab)
```

    ## spc_tbl_ [768 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
    ##  $ Pregnancies             : num [1:768] 6 1 8 1 0 5 3 10 2 8 ...
    ##  $ Glucose                 : num [1:768] 148 85 183 89 137 116 78 115 197 125 ...
    ##  $ BloodPressure           : num [1:768] 72 66 64 66 40 74 50 0 70 96 ...
    ##  $ SkinThickness           : num [1:768] 35 29 0 23 35 0 32 0 45 0 ...
    ##  $ Insulin                 : num [1:768] 0 0 0 94 168 0 88 0 543 0 ...
    ##  $ BMI                     : num [1:768] 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...
    ##  $ DiabetesPedigreeFunction: num [1:768] 0.627 0.351 0.672 0.167 2.288 ...
    ##  $ Age                     : num [1:768] 50 31 32 21 33 30 26 29 53 54 ...
    ##  $ Outcome                 : num [1:768] 1 0 1 0 1 0 1 0 1 1 ...
    ##  - attr(*, "spec")=
    ##   .. cols(
    ##   ..   Pregnancies = col_double(),
    ##   ..   Glucose = col_double(),
    ##   ..   BloodPressure = col_double(),
    ##   ..   SkinThickness = col_double(),
    ##   ..   Insulin = col_double(),
    ##   ..   BMI = col_double(),
    ##   ..   DiabetesPedigreeFunction = col_double(),
    ##   ..   Age = col_double(),
    ##   ..   Outcome = col_double()
    ##   .. )
    ##  - attr(*, "problems")=<externalptr>

**Answer: We can see that all the variables are continuous, which is a
nice perk initially. The only factor we have is outcome, which is
already set to 0/1. There are also no missing or NA values, so the data
set is very clean.**

------------------------------------------------------------------------

Question 2 — (5 points)  
Normalize the explanatory variables using min-max normalization.  

``` r
# pregnancies
min.preg <- min(diab$Pregnancies)
max.preg <- max(diab$Pregnancies)
diab$preg = (diab$Pregnancies - min.preg)/(max.preg- min.preg)

# glucose
min.gluc <- min(diab$Glucose)
max.gluc <- max(diab$Glucose)
diab$gluc = (diab$Glucose - min.gluc)/(max.gluc- min.gluc)

# bloodpressure
min.bp <- min(diab$BloodPressure)
max.bp <- max(diab$BloodPressure)
diab$bp = (diab$BloodPressure - min.bp)/(max.bp- min.bp)

# skinthickness
min.skin <- min(diab$SkinThickness)
max.skin <- max(diab$SkinThickness)
diab$skin = (diab$SkinThickness - min.skin)/(max.skin- min.skin)

# insulin
min.ins <- min(diab$Insulin)
max.ins <- max(diab$Insulin)
diab$ins = (diab$Insulin - min.ins)/(max.ins- min.ins)

# BMI
min.bmi <- min(diab$BMI)
max.bmi <- max(diab$BMI)
diab$bmi = (diab$BMI - min.bmi)/(max.bmi- min.bmi)

# diabetespedigreefunction
min.ped <- min(diab$DiabetesPedigreeFunction)
max.ped <- max(diab$DiabetesPedigreeFunction)
diab$ped = (diab$DiabetesPedigreeFunction - min.ped)/(max.ped- min.ped)

# age
min.years <- min(diab$Age)
max.years <- max(diab$Age)
diab$years = (diab$Age - min.years)/(max.years- min.years)
```

**Answer: I utilized the min/max normalization to normalize all of the
independent variables on a 0 to 1 scale. **

------------------------------------------------------------------------

Question 3 — (5 points)  
Split the data into a training set and a test set i.e. perform an 80/20
split; 80% of the data should be designated as the training data and 20%
as the test data.  

``` r
set.seed(1)
diab.train <- sample(1:nrow(diab),nrow(diab)*0.8)
diab.test <- setdiff(1:nrow(diab),diab.train)

# create the training and testing sample data frames
diab.trainset <- diab[diab.train,]
diab.testset <- diab[diab.test,]

# removed normal values, only normalized 

diab.testset <- diab.testset[-(1:8)]
diab.trainset <- diab.trainset[-(1:8)]
```

**Answer: I utilized a random sample of rows to select for approximately
80% of the rows in the diabetes csv to be put into the training set
data.frame and the rest in the testing set data.frame. **

------------------------------------------------------------------------

Question 4 — (25 points)  
Create a function called knn_predict(). The function should accept the
following as input: the training set, the test set and the value of k.
For example knn_predict(train.data, test.data, k).  

• Implement the logic for the k-nn algorithm from scratch (without using
any libraries). There is an example in the lecture series on Canvas. The
goal of your k-nn algorithm is to predict the Outcome (i.e. whether or
not the patient has diabetes) using the explanatory variables.  

• The function should return a list/vector of predictions for all
observations in the test set.  

``` r
# to find k which is ~25
sqrt(614)
```

    ## [1] 24.77902

``` r
sqrt(154)
```

    ## [1] 12.40967

``` r
sqrt(768)
```

    ## [1] 27.71281

``` r
## kdist distance function ================================================================
kdist <- function(p,q){
    d <- 0
  for (i in 1:length(p)){
    d <- d + (p[i] - q[i])^2
  }
  kdist <- sqrt(d)
}

#testing while building functions to ensure correct
# tests first test against second row of training set
#p <- as.numeric(diab.testset[1,c(2:9)])
#p
#q <- as.numeric(diab.trainset[2,c(2:9)])
#q
#test1 <- kdist(p,q)
#test1

## kneighbors ================================================================
kneighbors <- function(train,u,tstrw){
  m <- nrow(train)
  ds <- numeric(m)
  q <- as.numeric(u[tstrw,c(2:9)])
  for (i in 1:m){
    p <- train[i,c(2:9)]
    #q <- u[i,c(2:9)]
    ds[i] <- kdist(p,q)
  }
  # acheived the unlisting
  kneighbors <- unlist(ds)
  #kneighbors <- ds
}

#testing while building functions to ensure correct
tk <- kneighbors(diab.trainset,diab.testset,1)
tk
```

    ##   [1] 1.0071397 0.7173563 1.0523676 0.8572365 0.9973403 1.0910105 0.4410302
    ##   [8] 0.7486964 1.0602304 0.9624549 0.8138563 0.8203350 1.0386366 0.9319537
    ##  [15] 1.0456131 1.1188577 0.9682378 1.1165384 0.9251674 0.9931364 0.9762381
    ##  [22] 0.7338598 0.8262715 0.9815624 0.9827576 0.7097425 1.0424342 1.0828031
    ##  [29] 1.1365755 0.6802550 1.2973315 0.9474020 0.8554628 0.8049339 0.9922641
    ##  [36] 0.9418550 0.7944070 0.6900976 0.9855233 0.9631478 0.9617204 1.0525452
    ##  [43] 0.9364203 0.8920080 1.0540328 1.0574681 0.9966403 0.8924202 0.8833022
    ##  [50] 1.0081208 1.0126413 0.9122110 0.9913478 0.5746418 1.0056049 0.8902000
    ##  [57] 0.8917493 1.0121941 0.8167085 1.1437904 0.9982509 0.7148336 1.0192579
    ##  [64] 0.9681268 0.9826017 0.9212485 0.8772125 0.9698293 1.1617774 1.0759706
    ##  [71] 0.8982894 0.9115498 0.9414110 0.4928696 1.0075543 0.6564053 0.8963877
    ##  [78] 0.6583978 1.0204361 0.9909599 1.0259816 0.8665322 0.9148725 1.0532667
    ##  [85] 0.8533683 0.9811795 0.8111387 0.9652580 0.8790537 0.8254693 0.9236001
    ##  [92] 1.0033734 0.9111380 1.0712845 0.9294068 1.0573402 0.8660970 1.1503816
    ##  [99] 0.8077335 0.8020754 0.7560362 0.9143392 0.7636528 0.7688033 1.2623300
    ## [106] 1.0191761 1.0324653 0.8313539 0.9211166 1.0183799 1.0029905 0.8391628
    ## [113] 0.9562640 0.7602988 1.1710516 1.1235049 0.9471490 0.5561560 0.8576211
    ## [120] 0.9421454 1.0500162 1.0120628 1.0700891 0.9843936 0.8979574 0.8766894
    ## [127] 0.9664413 1.0532894 0.9325699 1.0710854 1.0828627 0.9186832 0.9722579
    ## [134] 0.9574575 1.0023225 0.9422235 0.9151063 1.1047503 0.7740295 0.8605485
    ## [141] 1.2287353 0.8845031 0.9107663 0.7984804 1.0211741 1.1006583 0.7257035
    ## [148] 0.9286040 1.1890414 0.9353950 0.6876315 0.9549105 0.6563879 1.0374230
    ## [155] 0.9350399 0.8051340 0.7213679 1.0217529 0.6086094 0.8393709 1.0665399
    ## [162] 1.0850773 1.1557356 0.8834546 0.7991046 0.7514284 0.8952754 0.9016819
    ## [169] 1.1715396 0.7781858 0.7577152 0.7460379 0.8785381 0.8170298 1.0106204
    ## [176] 0.8526986 0.9393434 0.7765927 0.9226489 0.8169886 0.7796352 0.5952974
    ## [183] 0.6557583 0.9211743 0.7553813 0.9104890 0.9686137 0.9285433 0.5757892
    ## [190] 0.9049028 0.9363688 0.9743606 0.9350261 0.9456877 1.1379507 0.7995706
    ## [197] 0.9589968 0.5945049 0.7984618 0.7873682 0.9512381 0.8344472 0.9177416
    ## [204] 0.9175689 1.0217341 1.0355691 0.8629446 0.8514371 0.6061316 0.9350476
    ## [211] 0.7831660 0.9243532 0.8425344 0.8746529 1.1016262 0.9001162 0.9753490
    ## [218] 0.9323781 1.0550133 1.0139896 0.9813472 1.0156765 1.0205523 0.9600833
    ## [225] 0.7641394 0.9072347 0.8480699 0.6520892 0.7810636 1.1010612 0.9360835
    ## [232] 0.9223764 0.9330325 1.0365682 1.0128071 0.8090284 0.6025823 1.0956186
    ## [239] 0.8803217 0.9940871 0.9989850 0.7839422 1.0381901 1.0670416 0.9418105
    ## [246] 1.1219096 0.8189673 1.0075853 1.0431710 1.0079904 1.1761845 0.9567138
    ## [253] 1.2062152 0.9084727 0.9854951 0.8734081 1.0360663 0.9649928 0.9045197
    ## [260] 0.9848855 0.9881222 0.9193379 0.7837141 0.7093432 0.8865149 0.7275630
    ## [267] 0.9555325 1.1449490 1.0817038 0.8874259 0.8339591 0.4809063 0.8886481
    ## [274] 0.7506828 0.7712999 1.0393535 0.9416454 0.8118150 0.6835381 0.6018080
    ## [281] 0.8993201 1.0828109 0.9498599 0.7102066 1.0232481 0.9670705 0.9518051
    ## [288] 0.9022121 1.1198094 0.7991060 0.8124243 1.0124594 0.8889885 1.1229219
    ## [295] 0.7740052 0.8985491 1.0213527 0.9416840 0.8024688 0.7558287 0.9657598
    ## [302] 0.8857401 1.0637552 0.8834799 1.0711857 1.1102582 0.9180190 0.9141817
    ## [309] 0.6983937 0.8661152 0.7387234 0.8850773 1.0590275 0.5742419 0.8626654
    ## [316] 0.5385681 1.0009676 0.8071810 0.9480342 0.7024845 1.0141833 1.2020040
    ## [323] 0.9969889 1.0862065 0.9781420 1.2507209 0.6885931 1.0253071 0.7272555
    ## [330] 1.0014586 0.9482792 0.7794890 1.0106656 1.0232303 0.7581327 0.7111148
    ## [337] 0.9021023 1.1392794 1.3362712 0.6618654 0.9480840 1.0086391 0.6359203
    ## [344] 0.8464553 0.7535741 0.9398653 0.9976962 0.8454280 0.7389377 1.1455287
    ## [351] 0.5887735 0.9647988 0.8040901 0.9940261 0.9250378 0.6884745 0.9623817
    ## [358] 0.4807767 1.0883954 0.9324315 0.9726131 1.1121264 0.8363625 0.9823888
    ## [365] 1.0879290 1.0233403 0.8166964 0.9156024 0.9222796 0.9728280 0.9897882
    ## [372] 1.1029827 0.8667226 0.9166376 1.0951804 0.7470271 0.9599659 0.7599549
    ## [379] 0.9431422 0.9142350 0.9480139 0.8877577 1.1599243 0.9701157 0.7916515
    ## [386] 1.3099307 0.7818420 1.0336666 0.9507850 1.0729568 0.9888730 0.9989544
    ## [393] 0.5204024 0.7837388 0.9798549 0.8761308 1.1058092 0.9830869 0.8751795
    ## [400] 0.8200689 1.0139564 0.6797954 0.6379957 0.5145760 0.7869295 1.0187571
    ## [407] 0.7349156 0.9235882 0.7446073 0.7016215 0.7474202 0.9055999 0.9849674
    ## [414] 1.2264816 0.8082963 0.8727284 1.0335312 0.8901946 0.6579417 0.5845096
    ## [421] 0.7938189 1.0186945 0.9868620 1.2867270 0.6967850 0.7072225 0.8733936
    ## [428] 0.9864947 1.3017392 0.9478702 0.5067737 0.9043154 0.6363196 0.7783423
    ## [435] 0.9819543 0.7523750 0.9497925 0.9722947 1.0036504 0.7824433 1.1213300
    ## [442] 0.9474570 0.9159764 0.9180630 0.8787354 1.1333857 0.9330499 0.8118864
    ## [449] 0.8640497 0.9356526 1.1447264 0.6623615 0.8102801 1.2446323 0.6298094
    ## [456] 0.6055608 0.6480049 0.9570119 0.9363193 0.8451919 0.6056581 0.6645939
    ## [463] 0.9206778 1.0331402 0.7978550 1.0518129 1.0044795 0.5720940 0.9919678
    ## [470] 0.7650478 0.8775640 0.7438160 0.8114282 0.9961896 0.6628164 1.0365366
    ## [477] 0.7057286 0.9019166 0.7522603 0.7590676 0.9410201 0.7308055 0.8417289
    ## [484] 0.9892839 1.1932385 0.8917217 1.2066745 0.8680377 0.9350714 0.9648317
    ## [491] 0.9741545 1.1765559 1.0669482 0.7370370 0.9819888 0.5293181 1.0071767
    ## [498] 1.0071379 0.9769609 0.7666537 0.7820473 0.9590754 0.9307729 0.5329669
    ## [505] 1.1077317 0.8021431 1.0382008 0.9534969 1.2012011 0.9844206 1.0740901
    ## [512] 0.9687289 0.5335732 0.7893697 0.9611486 0.9146201 0.8916465 0.7424031
    ## [519] 1.0238979 0.9364735 1.1102823 0.9781276 0.8349280 0.9185770 0.9615966
    ## [526] 0.8175332 0.9422008 1.0161207 1.0889345 1.0663790 0.5435363 0.9172662
    ## [533] 0.8277498 1.3285976 0.9404858 0.7542339 1.1233385 0.9897618 0.9733746
    ## [540] 0.9113335 0.8100681 0.9766947 0.9103934 0.8598679 0.9182192 0.9924985
    ## [547] 0.8415659 0.9318693 0.8755261 1.0567262 0.8905083 0.8710244 1.0531279
    ## [554] 0.8795680 1.0374746 0.7429894 1.0542537 0.8364444 0.7324563 1.3306103
    ## [561] 0.6272632 0.7988449 0.6745379 0.8486947 0.8467966 0.5179301 0.9238166
    ## [568] 0.9669540 0.7433506 0.7709744 1.1477167 0.9614274 0.9788643 0.8767122
    ## [575] 0.9899563 1.1125346 0.8185580 0.9148967 1.0835915 0.9746990 0.8703825
    ## [582] 0.9439696 1.0595534 0.9044555 0.7985918 0.9140131 0.5491177 1.0420805
    ## [589] 0.9506838 0.9011125 0.8547111 0.9408271 1.1243172 1.0692344 1.0898360
    ## [596] 0.9565916 0.9133553 0.9473927 0.7550706 1.1130210 0.9282808 0.7384547
    ## [603] 1.3431126 0.6398920 0.8637012 0.8724669 0.8569764 1.1422487 0.9422032
    ## [610] 1.0538200 0.8280216 1.0275829 0.9886993 0.9905915

``` r
otk <- order(tk)
ktest <- 25
otk[1:ktest]
```

    ##  [1]   7 358 272  74 431 404 566 393 496 504 513 316 531 587 118 468 314  54 189
    ## [20] 420 351 198 182 280 237

``` r
## k closest function ================================================================
kclosest <- function(neighbors,k){
  ordered.neighbors <- order(neighbors)
  k.closest <- ordered.neighbors[1:k]
}

#testing while building functions to ensure correct
tkclos <- kclosest(tk,25)
tkclos
```

    ##  [1]   7 358 272  74 431 404 566 393 496 504 513 316 531 587 118 468 314  54 189
    ## [20] 420 351 198 182 280 237

``` r
## kmode function ================================================================
kmode <- function(x){
  ux <- unique(x)
  ux[which.max(tabulate(match(x,ux)))]
}

#testing while building functions to ensure correct
kmode(tkclos)
```

    ## [1] 7

``` r
diab.trainset$Outcome[tkclos]
```

    ##  [1] 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1

``` r
# predicts the outcome of the first row of the testing set
kmode(diab.trainset$Outcome[tkclos])
```

    ## [1] 1

``` r
## k nearest numbers function ================================================================
# this predicts 1 number
knn_predict <- function(train, u, k){
  nb <- kneighbors(train, u, 1)
  f <- kclosest(nb, k)
  knn <- kmode(train$Outcome[f])
}

#test 
nn <- knn_predict(diab.trainset, diab.testset, 25)
nn
```

    ## [1] 1

``` r
## k nearest numbers function ================================================================
## k nearest numbers function ================================================================
#testing while building functions to ensure correct
knn_pred_all <- function(train, u, k){
  ru <- nrow(u)
  run <- numeric(ru)
  for (i in 1:ru){
    nb <- kneighbors(train, u, i)
    f <- kclosest(nb, k)
    knn <- kmode(train$Outcome[f])  
    run[i] <- knn
  } 
  knn_pred_all <- run
}

#test
nnar <- knn_pred_all(diab.trainset, diab.testset, 25)
nnar
```

    ##   [1] 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
    ##  [38] 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
    ##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0
    ## [112] 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0
    ## [149] 0 1 1 0 1 0

**Answer: Above is a long continuous process of building the functions
just as we did in the learning module. I built helper functions: kdist,
kneighbors, kclosest, and kmode. These helped me achieve writing a
function knn_pred_all that loops through the test data frame, the
training data frame, calculates the knn for each test value related to
all training data points and outputs a vector of the decisions.**

------------------------------------------------------------------------

Question 5 — (10 points)  
Demonstrate that the knn_predict() function works and use it to make
predictions for the test set. You can determine a suitable value of k
for your demonstration. After which, analyze the results that were
returned from the function using a confusion matrix. Explain the
results. Note: refer to the ‘Useful Resources’ section for more
information on building a confusion matrix in R.  

``` r
# Below I simply just call the function I crafted in question 4
#nnar <- knn_pred_all(diab.trainset, diab.testset, 25)
nnar
```

    ##   [1] 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
    ##  [38] 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
    ##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0
    ## [112] 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0
    ## [149] 0 1 1 0 1 0

``` r
# here I added the vector as a column to my test set just to be able to compare manually to the actual outcomes. 
diab.testset$knn25 <- nnar
diab.testset %>% 
  select(Outcome,knn25)
```

    ## # A tibble: 154 × 2
    ##    Outcome knn25
    ##      <dbl> <dbl>
    ##  1       1     1
    ##  2       1     0
    ##  3       1     1
    ##  4       1     1
    ##  5       1     1
    ##  6       1     0
    ##  7       1     0
    ##  8       1     1
    ##  9       0     0
    ## 10       0     0
    ## # ℹ 144 more rows

``` r
# confusion matrix for k=25
k25nncm <- confusionMatrix(data = as.factor(diab.testset$knn25), reference = as.factor(diab.testset$Outcome))
k25nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 95 28
    ##          1  8 23
    ##                                           
    ##                Accuracy : 0.7662          
    ##                  95% CI : (0.6914, 0.8306)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.005473        
    ##                                           
    ##                   Kappa : 0.4143          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.001542        
    ##                                           
    ##             Sensitivity : 0.9223          
    ##             Specificity : 0.4510          
    ##          Pos Pred Value : 0.7724          
    ##          Neg Pred Value : 0.7419          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6169          
    ##    Detection Prevalence : 0.7987          
    ##       Balanced Accuracy : 0.6867          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

**Answer: I chose to use the square root of the amount of training data
points which ended up being around 25. I added the vector to my test
data frame and built a confusion matrix against the true values. It
appears my knn algorithm and model has a 76.62 % accuracy rate with a
significant pvalue. I belive this to be statistically acceptable.
However, this depends on the severity of consequences of the decision
that this data helps make. This is about risk factors for predicting
diabetes. While this is more important than other things in life, I
would ideally like to see a higher accuracy rate. However, we hare over
3/4 accurate which is much better than a guess. **

------------------------------------------------------------------------

Question 6 — (+5 bonus points)  
Repeat question 5 and perform an experiment using different values of k.
Ensure that you try at least 5 different values of k and display the
confusion matrix from each attempt. Which value of k produced the most
accurate predictions?  

``` r
## k == 50 ---------------------------------------------------
knn50 <- knn_pred_all(diab.trainset, diab.testset, 50)
# added to df
diab.testset$knn50 <- knn50
#k50 confusion matrix
k50nncm <- confusionMatrix(data = as.factor(diab.testset$knn50), reference = as.factor(diab.testset$Outcome))
k50nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 99 26
    ##          1  4 25
    ##                                           
    ##                Accuracy : 0.8052          
    ##                  95% CI : (0.7337, 0.8645)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.0001269       
    ##                                           
    ##                   Kappa : 0.5065          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.0001260       
    ##                                           
    ##             Sensitivity : 0.9612          
    ##             Specificity : 0.4902          
    ##          Pos Pred Value : 0.7920          
    ##          Neg Pred Value : 0.8621          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6429          
    ##    Detection Prevalence : 0.8117          
    ##       Balanced Accuracy : 0.7257          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 49 ---------------------------------------------------
knn49 <- knn_pred_all(diab.trainset, diab.testset, 49)
# added to df
diab.testset$knn49 <- knn49
#k50 confusion matrix
k49nncm <- confusionMatrix(data = as.factor(diab.testset$knn49), reference = as.factor(diab.testset$Outcome))
k49nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 99 28
    ##          1  4 23
    ##                                           
    ##                Accuracy : 0.7922          
    ##                  95% CI : (0.7195, 0.8533)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.0005149       
    ##                                           
    ##                   Kappa : 0.4677          
    ##                                           
    ##  Mcnemar's Test P-Value : 4.785e-05       
    ##                                           
    ##             Sensitivity : 0.9612          
    ##             Specificity : 0.4510          
    ##          Pos Pred Value : 0.7795          
    ##          Neg Pred Value : 0.8519          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6429          
    ##    Detection Prevalence : 0.8247          
    ##       Balanced Accuracy : 0.7061          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 10  ---------------------------------------------------
knn10 <- knn_pred_all(diab.trainset, diab.testset, 10)
# added to df
diab.testset$knn10 <- knn10
#k50 confusion matrix
k10nncm <- confusionMatrix(data = as.factor(diab.testset$knn10), reference = as.factor(diab.testset$Outcome))
k10nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 90 21
    ##          1 13 30
    ##                                          
    ##                Accuracy : 0.7792         
    ##                  95% CI : (0.7054, 0.842)
    ##     No Information Rate : 0.6688         
    ##     P-Value [Acc > NIR] : 0.001801       
    ##                                          
    ##                   Kappa : 0.4811         
    ##                                          
    ##  Mcnemar's Test P-Value : 0.229949       
    ##                                          
    ##             Sensitivity : 0.8738         
    ##             Specificity : 0.5882         
    ##          Pos Pred Value : 0.8108         
    ##          Neg Pred Value : 0.6977         
    ##              Prevalence : 0.6688         
    ##          Detection Rate : 0.5844         
    ##    Detection Prevalence : 0.7208         
    ##       Balanced Accuracy : 0.7310         
    ##                                          
    ##        'Positive' Class : 0              
    ## 

``` r
## k == 9  ---------------------------------------------------
knn9 <- knn_pred_all(diab.trainset, diab.testset, 9)
# added to df
diab.testset$knn9 <- knn9
#k50 confusion matrix
k9nncm <- confusionMatrix(data = as.factor(diab.testset$knn9), reference = as.factor(diab.testset$Outcome))
k9nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 89 23
    ##          1 14 28
    ##                                           
    ##                Accuracy : 0.7597          
    ##                  95% CI : (0.6844, 0.8248)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.009073        
    ##                                           
    ##                   Kappa : 0.4324          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.188445        
    ##                                           
    ##             Sensitivity : 0.8641          
    ##             Specificity : 0.5490          
    ##          Pos Pred Value : 0.7946          
    ##          Neg Pred Value : 0.6667          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.5779          
    ##    Detection Prevalence : 0.7273          
    ##       Balanced Accuracy : 0.7065          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 15 ---------------------------------------------------
knn15 <- knn_pred_all(diab.trainset, diab.testset, 15)
# added to df
diab.testset$knn15 <- knn15
#k50 confusion matrix
k15nncm <- confusionMatrix(data = as.factor(diab.testset$knn15), reference = as.factor(diab.testset$Outcome))
k15nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 93 25
    ##          1 10 26
    ##                                           
    ##                Accuracy : 0.7727          
    ##                  95% CI : (0.6984, 0.8363)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.003194        
    ##                                           
    ##                   Kappa : 0.4458          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.017960        
    ##                                           
    ##             Sensitivity : 0.9029          
    ##             Specificity : 0.5098          
    ##          Pos Pred Value : 0.7881          
    ##          Neg Pred Value : 0.7222          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6039          
    ##    Detection Prevalence : 0.7662          
    ##       Balanced Accuracy : 0.7064          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 40 ---------------------------------------------------
knn40 <- knn_pred_all(diab.trainset, diab.testset, 40)
# added to df
diab.testset$knn40 <- knn40
#k50 confusion matrix
k40nncm <- confusionMatrix(data = as.factor(diab.testset$knn40), reference = as.factor(diab.testset$Outcome))
k40nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 98 27
    ##          1  5 24
    ##                                           
    ##                Accuracy : 0.7922          
    ##                  95% CI : (0.7195, 0.8533)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.0005149       
    ##                                           
    ##                   Kappa : 0.4736          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.0002054       
    ##                                           
    ##             Sensitivity : 0.9515          
    ##             Specificity : 0.4706          
    ##          Pos Pred Value : 0.7840          
    ##          Neg Pred Value : 0.8276          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6364          
    ##    Detection Prevalence : 0.8117          
    ##       Balanced Accuracy : 0.7110          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 35 ---------------------------------------------------
knn35 <- knn_pred_all(diab.trainset, diab.testset, 35)
# added to df
diab.testset$knn35 <- knn35
#k50 confusion matrix
k35nncm <- confusionMatrix(data = as.factor(diab.testset$knn35), reference = as.factor(diab.testset$Outcome))
k35nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 95 28
    ##          1  8 23
    ##                                           
    ##                Accuracy : 0.7662          
    ##                  95% CI : (0.6914, 0.8306)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.005473        
    ##                                           
    ##                   Kappa : 0.4143          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.001542        
    ##                                           
    ##             Sensitivity : 0.9223          
    ##             Specificity : 0.4510          
    ##          Pos Pred Value : 0.7724          
    ##          Neg Pred Value : 0.7419          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6169          
    ##    Detection Prevalence : 0.7987          
    ##       Balanced Accuracy : 0.6867          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 5 ---------------------------------------------------
knn5 <- knn_pred_all(diab.trainset, diab.testset, 5)
# added to df
diab.testset$knn5 <- knn5
#k50 confusion matrix
k5nncm <- confusionMatrix(data = as.factor(diab.testset$knn5), reference = as.factor(diab.testset$Outcome))
k5nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 88 25
    ##          1 15 26
    ##                                           
    ##                Accuracy : 0.7403          
    ##                  95% CI : (0.6635, 0.8075)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.03415         
    ##                                           
    ##                   Kappa : 0.3831          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.15473         
    ##                                           
    ##             Sensitivity : 0.8544          
    ##             Specificity : 0.5098          
    ##          Pos Pred Value : 0.7788          
    ##          Neg Pred Value : 0.6341          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.5714          
    ##    Detection Prevalence : 0.7338          
    ##       Balanced Accuracy : 0.6821          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 75 ---------------------------------------------------
knn75 <- knn_pred_all(diab.trainset, diab.testset, 75)
# added to df
diab.testset$knn75 <- knn75
#k50 confusion matrix
k75nncm <- confusionMatrix(data = as.factor(diab.testset$knn75), reference = as.factor(diab.testset$Outcome))
k75nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 98 30
    ##          1  5 21
    ##                                           
    ##                Accuracy : 0.7727          
    ##                  95% CI : (0.6984, 0.8363)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.003194        
    ##                                           
    ##                   Kappa : 0.4145          
    ##                                           
    ##  Mcnemar's Test P-Value : 4.976e-05       
    ##                                           
    ##             Sensitivity : 0.9515          
    ##             Specificity : 0.4118          
    ##          Pos Pred Value : 0.7656          
    ##          Neg Pred Value : 0.8077          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.6364          
    ##    Detection Prevalence : 0.8312          
    ##       Balanced Accuracy : 0.6816          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## k == 13 ---------------------------------------------------
knn13 <- knn_pred_all(diab.trainset, diab.testset, 13)
# added to df
diab.testset$knn13 <- knn13
#k50 confusion matrix
k13nncm <- confusionMatrix(data = as.factor(diab.testset$knn13), reference = as.factor(diab.testset$Outcome))
k13nncm
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 91 21
    ##          1 12 30
    ##                                           
    ##                Accuracy : 0.7857          
    ##                  95% CI : (0.7124, 0.8477)
    ##     No Information Rate : 0.6688          
    ##     P-Value [Acc > NIR] : 0.0009805       
    ##                                           
    ##                   Kappa : 0.4937          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.1637344       
    ##                                           
    ##             Sensitivity : 0.8835          
    ##             Specificity : 0.5882          
    ##          Pos Pred Value : 0.8125          
    ##          Neg Pred Value : 0.7143          
    ##              Prevalence : 0.6688          
    ##          Detection Rate : 0.5909          
    ##    Detection Prevalence : 0.7273          
    ##       Balanced Accuracy : 0.7359          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
## visualize results --------------------------------------------
knum <- c(5, 9, 10, 13, 15, 25, 35, 40, 49, 50, 75)
kacc <- c()
kprc <- c()
krec <- c()

kacc[1] <- k5nncm$overall['Accuracy']
kacc[2] <- k9nncm$overall['Accuracy']
kacc[3] <- k10nncm$overall['Accuracy']
kacc[4] <- k13nncm$overall['Accuracy']
kacc[5] <- k15nncm$overall['Accuracy']
kacc[6] <- k25nncm$overall['Accuracy']
kacc[7] <- k35nncm$overall['Accuracy']
kacc[8] <- k40nncm$overall['Accuracy']
kacc[9] <- k49nncm$overall['Accuracy']
kacc[10] <- k50nncm$overall['Accuracy']
kacc[11] <- k75nncm$overall['Accuracy']

# create a data frame for ease of use
knn_stat <- data.frame(knum,kacc)

# ggplot to visualize the accuracy and the k value for our knn model
ggplot(knn_stat, aes(x=knum, y=kacc)) + geom_line() + geom_point() + ylab("Totaly Accuracy") + xlab("K Value") + ggtitle("K Values vs Model Accuracy") + scale_x_continuous(n.breaks = 15)
```

![](DA5020.A11.John.Regan_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

**Answer: I found the results of the kvalues to be surprising. After
reading more in the text I found that I should make the k value odd so
there are no ties when the voting takes place. I repeated all the larger
k numbers with similar odd numbers. After noticing the peak at 50, I
decided to go much higher, although computationally expensive, and aware
that it is unreasonably high, I just wanted to see what would happen.
Since the square root of our training data is around 25, which seems to
be ubiquitously recommended as the desired k value, I believe it would
be somewhere around that number. However, we see a peak in accurary
around 10. I decided to run again with k=13 to see what was happening
around that area. I think that the k value, although peaking at 50,
would be a mistake as large k values can lead to underfitting but small
k values can lead to overfitting. We do find a valley right at the
square root of the training data and perhaps that is the best since the
accuracy is still relatively high but not peaked, indicating some other
issues are occuring. **

------------------------------------------------------------------------
